# LLM

**Offloading to utilize memory from CPU and disk**
* FlexGen: High-throughput Generative Inference of Large Language Models with a Single GPU (ICML'23) [[paper]() | [review](https://github.com/wonkyoc/LLM/reviews/flexgen.md)]

**Distributed Inference Systems**
* ORCA: A Distributed Serving System for Transformer-Based Generative Models (OSDI'22) [[paper](https://www.usenix.org/system/files/osdi22-yu.pdf) | [review]()]
* Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning (OSDI'22) [[paper](https://www.usenix.org/system/files/osdi22-zheng-lianmin.pdf) | [review]()]

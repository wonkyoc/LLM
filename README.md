# LLM

**Offloading to utilize memory from CPU and disk**
* FlexGen: High-throughput Generative Inference of Large Language Models with a Single GPU (ICML'23) [[paper](https://arxiv.org/abs/2303.06865) | [review](reviews/flexgen.md)]

**Distributed Inference Systems**
* ORCA: A Distributed Serving System for Transformer-Based Generative Models (OSDI'22) [[paper](https://www.usenix.org/system/files/osdi22-yu.pdf) | [review](reviews/orca.md)]
* Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning (OSDI'22) [[paper](https://www.usenix.org/system/files/osdi22-zheng-lianmin.pdf) | [review](reviews/alpa.md)]

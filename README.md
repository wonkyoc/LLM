# LLM

**A technique for accelerating LLM inference**
* Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline (arxiv) [[paper](https://arxiv.org/abs/2305.13144) | [review](sequence-scheduling.md)]

**Offloading to utilize memory from CPU and disk**
* FlexGen: High-throughput Generative Inference of Large Language Models with a Single GPU (ICML'23) [[paper](https://arxiv.org/abs/2303.06865) | [review](reviews/flexgen.md)]

**Distributed Inference Systems**
* ORCA: A Distributed Serving System for Transformer-Based Generative Models (OSDI'22) [[paper](https://www.usenix.org/system/files/osdi22-yu.pdf) | [review](reviews/orca.md)]

**Model Parallelism**
* Beta: Statistical Multiplexing with Model Parallelism for Deep Learning Serving [[paper]() | [review](reviews/beta.md)]

**Parallel DL training systems**
* Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning (OSDI'22) [[paper](https://www.usenix.org/system/files/osdi22-zheng-lianmin.pdf) | [review](reviews/alpa.md)]

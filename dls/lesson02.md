

Three ingredients of a machine learning algorithm
* The hypothesis class: a set of parameters that describes how we map inputs to outputs
* The loss function: a function that specifies how "well" a given hypothesis (i.e., a choice of parameters) performs on the task of interest
* An optimization method: minimize the sum of losses over the training set

Linear hypothesis function
* Our hypothesis function maps inputs x <= R^n to k-dimentisonal vectors... 

Matrix batch notation
* often more convenient (and this is how you want to code thigns for efficiency)

Loss function -- classification error
* compare our h(x) and true value

Lass function -- softmax / cross-entropy loss

Regression?

Softmax?
